# Docker 기초 및 Spark 컨테이너화 정리

## 🐳 Docker 소개

Docker는 애플리케이션을 개발, 배포, 실행하기 위한 오픈 소스 플랫폼입니다. Docker를 사용하면 애플리케이션을 인프라에서 분리하여 소프트웨어를 빠르게 제공할 수 있습니다.

### 설치 및 기본 정보

- 📥 Docker 설치: https://www.docker.com/

### 주요 이점

### 1️⃣ 일관된 개발 및 운영 환경

- 모든 개발자와 서버에서 동일한 환경 보장
- OS, Python 버전, 라이브러리 의존성 등 일관성 유지

### 2️⃣ 컨테이너화 기술

- 프로젝트별로 독립된 환경 제공
- 의존성 충돌 문제 해결
- 격리된 실행 환경으로 보안성 향상

## 🛠️ Docker 사용법

### 1. Dockerfile 작성

Dockerfile은 Docker 이미지를 빌드하기 위한 지침이 포함된 텍스트 파일입니다.

```
FROM python:3.11.1

# Python 출력(stdout, stderr)이 터미널에 바로 전송되도록 설정
ENV PYTHONUNBUFFERED 1

# 개발 모드 설정을 위한 인자
ARG DEV=false

# 필요한 파일 복사
COPY requirements.txt /app/
COPY requirements.dev.txt /app/
COPY app /app/

# 작업 디렉토리 설정
WORKDIR /app

# 의존성 설치
RUN pip install -r requirements.txt

# 개발 모드일 경우에만 개발 의존성 설치
RUN if [ $DEV = true ]; then pip install -r requirements.dev.txt; fi

# 포트 노출
EXPOSE 8000

```

### 2. Docker Compose 작성

Docker Compose는 다중 컨테이너 Docker 애플리케이션을 정의하고 실행하기 위한 도구입니다.

```yaml
version: "3.9"

services:
  app:
    build:
      context: .
      args:
        - DEV=true
    ports:
      - "8000:8000"
    volumes:
      - ./app:/app
    command: >
      sh -c "python manage.py runserver 0.0.0.0:8000"
```

### 3. Docker Compose 구성 요소

### ⚙️ 설정

- **서비스 이름**: 컨테이너의 논리적 이름
- **포트 매핑**: 로컬 시스템과 Docker 컨테이너 간의 포트 연결
  - `"8000:8000"`: 로컬 8000 포트를 컨테이너의 8000 포트에 연결
- **볼륨 매핑**: 로컬 파일 시스템과 Docker 컨테이너 간의 디렉토리 연결
  - `./app:/app`: 로컬 app 디렉토리를 컨테이너의 /app 디렉토리에 마운트

### ▶️ 실행 명령어

- **이미지 빌드**: `docker compose build`
- **컨테이너 실행**: `docker compose up`
- **백그라운드 실행**: `docker compose up -d`
- **컨테이너 중지**: `docker compose down`

## 🔄 Apache Spark와 Docker 통합

### Spark 컨테이너화의 이점

1. 일관된 Spark 실행 환경 보장
2. 다양한 버전의 Spark를 격리된 환경에서 실행 가능
3. 클러스터 관리 간소화
4. 확장성 있는 배포 구조 제공

### Spark Docker 이미지 구성 예시

```
FROM openjdk:8-jdk

# Spark 및 Hadoop 버전 설정
ENV SPARK_VERSION=3.1.2
ENV HADOOP_VERSION=3.2

# Spark 설치
RUN wget <https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz> && \\
    tar -xvzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /spark && \\
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Python 설치
RUN apt-get update && apt-get install -y python3 python3-pip

# 환경 변수 설정
ENV SPARK_HOME=/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# 작업 디렉토리 설정
WORKDIR /app

# PySpark 및 필요한 라이브러리 설치
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# 스파크 마스터 및 워커 노드 포트 노출
EXPOSE 4040 7077 8080 8081

# 기본 명령어 설정
CMD ["spark-shell"]

```

### Spark 멀티 노드 Docker Compose 예시

```yaml
version: "3"

services:
  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
      - "7077:7077"

  spark-worker:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
```

## 🔑 핵심 요약

1. **Docker**는 애플리케이션 의존성과 실행 환경을 패키징하여 일관된 환경을 제공합니다.
2. **Dockerfile**은 이미지 빌드 지침을 포함하며, 기본 이미지 선택부터 명령어 실행까지 정의합니다.
3. **Docker Compose**는 여러 컨테이너를 정의하고 관리하는 도구로, 복잡한 애플리케이션의 구성을 단순화합니다.
4. **Spark와 Docker 통합**은 일관된 Spark 환경을 제공하고, 확장 가능한 클러스터 구성을 용이하게 합니다.

## 🚀 시작하기

1. Docker 설치
2. 프로젝트에 Dockerfile 생성
3. docker-compose.yml 작성
4. `docker compose up` 명령으로 환경 실행
